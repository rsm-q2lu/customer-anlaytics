{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pyrsm import gains, gains_plot, lift, lift_plot, confusion, profit_max, ROME_max\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('intuit75k_withstate.csv')\n",
    "data = pd.read_csv('../intuit75k_new.csv')\n",
    "data['label']=1\n",
    "data.loc[data.res1 == 'No','label'] = 0\n",
    "#onehot=['zip_state','sex']\n",
    "onehot=['sex']\n",
    "tostandard=['numords','last','dollars','sincepurch']\n",
    "othercol=['bizflag','owntaxprod','version1','upgraded','training','zip_bins']\n",
    "keep=onehot+tostandard+othercol+['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data: using one-hot encoding\n",
    "standardizer=preprocessing.StandardScaler()\n",
    "onehot_encoder=preprocessing.OneHotEncoder(sparse=False)\n",
    "subdata=data.loc[:,keep]\n",
    "data1=copy.copy(subdata)\n",
    "onehotdata=onehot_encoder.fit_transform(data1.loc[:,onehot])\n",
    "with_onehot = pd.DataFrame(onehotdata)\n",
    "with_onehot.columns=onehot_encoder.get_feature_names()\n",
    "without_onehot=data1.drop(columns=onehot)\n",
    "combind_data= pd.concat([without_onehot,with_onehot],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "\n",
    "\n",
    "X_train=combind_data.loc[combind_data.training==1].drop(columns='label').drop(columns='training')\n",
    "y_train=combind_data.loc[combind_data.training==1].label\n",
    "X_test=combind_data.loc[combind_data.training==0].drop(columns='label').drop(columns='training')\n",
    "y_test=combind_data.loc[combind_data.training==0].label\n",
    "\n",
    "# standarlization\n",
    "standardizer.fit(X_train[tostandard])\n",
    "X_train[tostandard] = standardizer.transform(X_train[tostandard])\n",
    "X_test[tostandard] = standardizer.transform(X_test[tostandard])\n",
    "\n",
    "\n",
    "#X=combind_data.drop(columns='label')\n",
    "#y=combind_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.concatenate((X_train, X_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model. Starting from 1 hidden layer\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation='relu',input_dim=X_train.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "model1_training=model.fit(X_train, y_train, epochs=100,callbacks=[early_stopping_monitor], verbose=False,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1735242588179452,\n",
       " 0.17042317731607529,\n",
       " 0.16944055260930743,\n",
       " 0.16874778256529854,\n",
       " 0.16890197452477046,\n",
       " 0.1743095489115942,\n",
       " 0.1687492711544037,\n",
       " 0.16815092720304217,\n",
       " 0.1696509228320349,\n",
       " 0.17107860312007722,\n",
       " 0.16879682648749578,\n",
       " 0.1679681167772838,\n",
       " 0.16827168679237367,\n",
       " 0.1688724715482621,\n",
       " 0.16852366757960546,\n",
       " 0.1680398109299796]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I enlarged the capacity of the model by adding another layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(50,activation='relu',input_dim=X_train.shape[1]))\n",
    "model2.add(Dense(15,activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17449952316284179,\n",
       " 0.1701170527935028,\n",
       " 0.16988816650708516,\n",
       " 0.16983868619373865,\n",
       " 0.16941280966713315,\n",
       " 0.17464681793394543,\n",
       " 0.16893626424812136,\n",
       " 0.16985435447806405,\n",
       " 0.16836157646065666,\n",
       " 0.1683837203071231,\n",
       " 0.16832149578843797,\n",
       " 0.16871232827504476,\n",
       " 0.1726594325417564,\n",
       " 0.17001824081511724,\n",
       " 0.17100101248423258]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "model2_training = model2.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model2_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16936201013837543,\n",
       " 0.17001591492834545,\n",
       " 0.170359029173851,\n",
       " 0.1696455550080254,\n",
       " 0.170324678057716]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=Sequential()\n",
    "model3.add(Dense(60,activation='relu',input_dim=X_train.shape[1]))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "model3_training = model2.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model3_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the loss of model 2 is worse than the model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we shrink the capacity of the nn by decreasing the nodes on the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17418206684929985,\n",
       " 0.17097678401356653,\n",
       " 0.17175154992512295,\n",
       " 0.17147149160362427,\n",
       " 0.1691337476571401,\n",
       " 0.1684588403474717,\n",
       " 0.16858727360907055,\n",
       " 0.16835953164100648,\n",
       " 0.16953019509996686,\n",
       " 0.16749064132145472,\n",
       " 0.169178090992428,\n",
       " 0.16753637280918304,\n",
       " 0.16851238024802434,\n",
       " 0.16935969447521937]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4=Sequential()\n",
    "model4.add(Dense(30,activation='relu',input_dim=X_train.shape[1]))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "model4_training = model3.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model4_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is not obviously less than model1. so we keep model1 and stop enlarge the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we try using softmax and accuracy to see if the model can generate better profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cate=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor=EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.173213343393235,\n",
       " 0.1705479483945029,\n",
       " 0.16983507405008588,\n",
       " 0.1704693902617409,\n",
       " 0.16886121462640308,\n",
       " 0.16914565891878947,\n",
       " 0.17083334933008468,\n",
       " 0.1684260798295339,\n",
       " 0.17306324926444464,\n",
       " 0.1684007877508799,\n",
       " 0.16916894172486804,\n",
       " 0.16859731895583016,\n",
       " 0.16900427661623274,\n",
       " 0.1691545715786162]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5=Sequential()\n",
    "model5.add(Dense(50,activation='relu',input_dim=X_train.shape[1]))\n",
    "model5.add(Dense(2, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model5_training = model5.fit(X_train, y_train_cate, epochs=100, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model5_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17502979739507038,\n",
       " 0.17071771400883085,\n",
       " 0.16909530858766464,\n",
       " 0.16872137498287929,\n",
       " 0.17419714268616268,\n",
       " 0.16878529275031318,\n",
       " 0.1761442494051797,\n",
       " 0.16845422232718696,\n",
       " 0.1681656505720956,\n",
       " 0.16799587097054436,\n",
       " 0.16941966664791108,\n",
       " 0.16932243266559782,\n",
       " 0.17653834667092277,\n",
       " 0.16829873877479917]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6=Sequential()\n",
    "model6.add(Dense(30,activation='relu',input_dim=X_train.shape[1]))\n",
    "model6.add(Dense(2, activation='softmax'))\n",
    "model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model6_training = model6.fit(X_train, y_train_cate, epochs=100, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model6_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss of model 6 is larger than model5, model5 is larger than model1, so using model1 as the final model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the profit, AUC of model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin=60\n",
    "cost=1.41\n",
    "breakeven_rate=cost/margin\n",
    "breakeven_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "preds=model.predict_proba(X_test)\n",
    "testdata=combind_data.loc[combind_data.training==0]\n",
    "testdata['deeplearning_result']=preds\n",
    "testdata['pred_click']=1\n",
    "testdata.loc[testdata['deeplearning_result']<breakeven_rate,'pred_click']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37623.66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp=testdata.loc[testdata.pred_click==1].loc[testdata.label==1]\n",
    "revenue=len(tp)*margin\n",
    "pred_true=sum(testdata.pred_click)\n",
    "totalcost=pred_true*cost\n",
    "profit=revenue-totalcost\n",
    "profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518452799091356"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test.values, deeplearning_result)\n",
    "auc_rf = metrics.auc(fpr, tpr)\n",
    "auc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "422604.09353066667"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaled profit\n",
    "total=763334\n",
    "testdata2=combind_data.loc[combind_data.training==0]\n",
    "testdata2['randomforest']=preds\n",
    "testdata2['pred_click']=1\n",
    "testdata2.loc[testdata2['randomforest']/2<breakeven_rate,'pred_click']=0\n",
    "tp=testdata2.loc[testdata2.pred_click==1].loc[testdata2.label==1]\n",
    "pred_true=sum(testdata2.pred_click)\n",
    "pred_true_rate=pred_true/len(testdata2)\n",
    "send_number=total*pred_true_rate\n",
    "adj_response_rate=len(tp)/pred_true/2\n",
    "exp_buyers=adj_response_rate*send_number\n",
    "totalcost=send_number*cost\n",
    "exp_profit=exp_buyers*margin-totalcost\n",
    "exp_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12456.66"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expected profit on the test set of res2\n",
    "send_number=len(testdata2)*pred_true_rate\n",
    "adj_response_rate=len(tp)/pred_true/2\n",
    "exp_buyers=adj_response_rate*send_number\n",
    "totalcost=send_number*cost\n",
    "exp_profit_test=exp_buyers*margin-totalcost\n",
    "exp_profit_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
